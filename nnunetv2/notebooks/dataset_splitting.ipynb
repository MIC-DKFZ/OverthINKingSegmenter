{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "import tifffile\n",
    "\n",
    "from batchgenerators.utilities.file_and_folder_operations import load_json, save_json, load_pickle, save_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnUNet_preprocessed = os.environ[\"nnUNet_preprocessed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split(seg):\n",
    "    seg = seg.reshape(seg.shape[-2:])\n",
    "    total = np.sum(seg>=0)\n",
    "    min_x = np.min(np.argwhere(seg>=0)[:,0])\n",
    "    max_x = np.max(np.argwhere(seg>=0)[:,0])\n",
    "    xy_split = dict()\n",
    "    x_split = {0: [min_x, None], 1: [None, None], 2: [None, None], 3: [None, None], 4: [None, max_x]}\n",
    "    current = min_x\n",
    "    for i in range(4):\n",
    "        for x in range(current+1, seg.shape[0]):\n",
    "            if np.sum(seg[current:x]>=0) >= total//5:\n",
    "                current = x_split[i][1] = x_split[i+1][0] = x\n",
    "                break\n",
    "    for i in range(5):\n",
    "        x_total = np.sum(seg[x_split[i][0]:x_split[i][1]]>=0)\n",
    "        min_y = np.min(np.argwhere(seg[x_split[i][0]:x_split[i][1]]>=0)[:,1])\n",
    "        max_y = np.max(np.argwhere(seg[x_split[i][0]:x_split[i][1]]>=0)[:,1])\n",
    "        y_split = {0: [min_y, None], 1: [None, None], 2: [None, None], 3: [None, None], 4: [None, max_y]}\n",
    "        current = min_y\n",
    "        for j in range(4):\n",
    "            for y in range(current+1, seg.shape[1]):\n",
    "                if np.sum(seg[x_split[i][0]:x_split[i][1], current:y]>=0) >= x_total//5:\n",
    "                    current = y_split[j][1] = y_split[j+1][0] = y\n",
    "                    break\n",
    "        for j in range(5):\n",
    "            xy_split[(i, j)] = (x_split[i], y_split[j])\n",
    "    return xy_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_save(old_task, new_task, case):\n",
    "    with np.load(old_task/\"nnUNetPlans_3d_fullres\"/f\"{case}.npz\") as f:\n",
    "        data = f[\"data\"]\n",
    "        seg = f[\"seg\"]\n",
    "    meta = load_pickle(old_task/\"nnUNetPlans_3d_fullres\"/f\"{case}.pkl\")\n",
    "    gt = tifffile.imread(old_task/\"gt_segmentations\"/f\"{case}.tif\")\n",
    "    bbox = meta[\"bbox_used_for_cropping\"]\n",
    "    gt = gt[slice(*bbox[1]), slice(*bbox[2])]\n",
    "    xy_split = find_split(seg)\n",
    "    for i, k in enumerate(xy_split.keys(), 1):\n",
    "        name = \"{:s}_{:02d}\".format(case, i)\n",
    "        x_slice = slice(*xy_split[k][0])\n",
    "        y_slice = slice(*xy_split[k][1])\n",
    "        data_slice = data[:, :, x_slice, y_slice]\n",
    "        seg_slice = seg[:, :, x_slice, y_slice]\n",
    "        np.savez(new_task/\"nnUNetPlans_3d_fullres\"/f\"{name}.npz\", data=data_slice, seg=seg_slice)\n",
    "        gt_slice = gt[x_slice, y_slice]\n",
    "        tifffile.imwrite(new_task/\"gt_segmentations\"/f\"{name}.tif\", gt_slice)\n",
    "        meta_slice = meta.copy()\n",
    "        meta_slice[\"shape_before_cropping\"] = data_slice.shape[-3:]\n",
    "        meta_slice[\"bbox_used_for_cropping\"] = [[0, 1], [0, data_slice.shape[-2]], [0, data_slice.shape[-1]]]\n",
    "        meta_slice[\"shape_after_cropping_and_before_resampling\"] = data_slice.shape[-3:]\n",
    "        meta_slice[\"class_locations\"] = {1: np.argwhere(seg_slice==1)}\n",
    "        meta_slice[\"data_locations\"] = np.argwhere(seg_slice>=0)\n",
    "        save_pickle(meta_slice, new_task/\"nnUNetPlans_3d_fullres\"/f\"{name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_dataset(old_task_name, new_task_name):\n",
    "    # create new task with data split into 25 parts each\n",
    "    old_task = Path(nnUNet_preprocessed/old_task_name)\n",
    "    new_task = Path(nnUNet_preprocessed/new_task_name)\n",
    "    new_task.mkdir(exist_ok=True)\n",
    "    (new_task/\"gt_segmentations\").mkdir(exist_ok=True)\n",
    "    (new_task/\"nnUNetPlans_3d_fullres\").mkdir(exist_ok=True)\n",
    "    dataset = load_json(old_task/\"dataset.json\")\n",
    "    dataset[\"name\"] = new_task_name\n",
    "    save_json(dataset, new_task/\"dataset.json\")\n",
    "    shutil.copy(old_task/\"dataset_fingerprint.json\", new_task/\"dataset_fingerprint.json\")\n",
    "    plans = load_json(old_task/\"nnUNetPlans.json\")\n",
    "    plans[\"dataset_name\"] = new_task_name\n",
    "    save_json(plans, new_task/\"nnUNetPlans.json\")\n",
    "    cases = sorted([c.name[:-4] for c in (old_task/\"gt_segmentations\").iterdir() if c.match(\"*.tif\")])\n",
    "    for case in cases:\n",
    "        split_and_save(old_task, new_task, case)\n",
    "\n",
    "    # create seeded split file for 5 fold cv training. every split contains 5 parts from each scroll\n",
    "    folds = [[] for _ in range(5)]\n",
    "    for case in cases:\n",
    "        new_case = [\"{:s}_{:02d}\".format(case, i) for i in range(1, 26)]\n",
    "        random.Random(42).shuffle(new_case)\n",
    "        for i in range(5):\n",
    "            folds[i].extend(new_case[i*5:(i+1)*5])\n",
    "    splits = []\n",
    "    splits.append({\"train\": sorted(folds[0]+folds[1]+folds[2]+folds[3]), \"val\": sorted(folds[4])})\n",
    "    splits.append({\"train\": sorted(folds[0]+folds[1]+folds[2]+folds[4]), \"val\": sorted(folds[3])})\n",
    "    splits.append({\"train\": sorted(folds[0]+folds[1]+folds[3]+folds[4]), \"val\": sorted(folds[2])})\n",
    "    splits.append({\"train\": sorted(folds[0]+folds[2]+folds[3]+folds[4]), \"val\": sorted(folds[1])})\n",
    "    splits.append({\"train\": sorted(folds[1]+folds[2]+folds[3]+folds[4]), \"val\": sorted(folds[0])})\n",
    "\n",
    "    save_json(splits, new_task/\"splits_final.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_task = \"Dataset800_vesuvius\"\n",
    "new_task = \"Dataset800_vesuvius_split\"\n",
    "create_new_dataset(old_task, new_task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
